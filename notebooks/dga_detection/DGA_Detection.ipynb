{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain Generation Algorithm (DGA) Detection\n",
    "\n",
    "## Authors\n",
    " - Gorkem Batmaz (NVIDIA) [gbatmaz@nvidia.com]\n",
    " - Bhargav Suryadevara (NVIDIA) [bsuryadevara@nvidia.com]\n",
    "\n",
    "## Development Notes\n",
    "* Developed using: RAPIDS v0.12.0 and CLX v0.12\n",
    "* Last tested using: RAPIDS v0.12.0 and CLX v0.12 on Jan 28, 2020\n",
    "\n",
    "## Table of Contents\n",
    "* Introduction\n",
    "* Data Importing\n",
    "* Data Preprocessing\n",
    "* Training and Evaluation\n",
    "* Inference\n",
    "* Conclusion\n",
    "\n",
    "## Introduction\n",
    "[Domain Generation Algorithms](https://en.wikipedia.org/wiki/Domain_generation_algorithm) (DGAs) are used to generate domain names that can be used by the malware to communicate with the command and control servers. IP addresses and static domain names can be easily blocked, and a DGA provides an easy method to generate a large number of domain names and rotate through them to circumvent traditional block lists. We will use a type of recurrent neural network called the [Gated Recurrent Unit](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21) (GRU) for this example. The [CLX](https://github.com/rapidsai/clx) and [RAPIDS](https://rapids.ai) libraries enable users train their models with up-to-date domain names representative of both benign and DGA generated strings. Using a CLX workflow, this capability could also be used in production. This notebook provides a view into the data science workflow to create a DGA detection implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import cudf\n",
    "import torch\n",
    "import s3fs\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, average_precision_score\n",
    "from clx.utils.data.dga_dataset import DGADataset\n",
    "from clx.utils.data.dataloader import DataLoader\n",
    "from clx.analytics.dga_detector import DGADetector\n",
    "from cuml.preprocessing.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download Input Dataset from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_CSV = \"benign_and_dga_domains.csv\"\n",
    "\n",
    "S3_BASE_PATH = \"rapidsai-data/cyber/clx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Benign and DGA dataset\n",
    "if not os.path.exists(INPUT_CSV):\n",
    "    fs = s3fs.S3FileSystem(anon=True)\n",
    "    fs.get(S3_BASE_PATH + \"/\" + INPUT_CSV, INPUT_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Input Dataset to GPU Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = cudf.read_csv(INPUT_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Train and Test Dataset\n",
    "We utilize the [`train_test_split` function](https://docs.rapids.ai/api/cuml/0.10/api.html#model-selection-and-data-splitting) from [cuML](https://github.com/rapidsai/cuml) and create a shuffled dataset for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_train, domain_test, type_train, type_test = train_test_split(input_df, 'type', train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(domain_df, type_series):\n",
    "    df = cudf.DataFrame()\n",
    "    df['domain'] = domain_df['domain'].reset_index(drop=True)\n",
    "    df['type'] = type_series.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = create_df(domain_train, type_train)\n",
    "test_df = create_df(domain_test, type_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DGADataset(train_df)\n",
    "test_dataset = DGADataset(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Batches using DataLoader\n",
    "We need to partition the input dataframe into one or more smaller dataframes as per the given batch size for training and testing of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10000\n",
    "train_dataloader = DataLoader(train_dataset, batchsize=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(test_dataset, batchsize=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have only benign and DGA (malicious) categoriesm, the number of domain types need to be set to 2 (`N_DOMAIN_TYPE=2`). Vocabulary size(`CHAR_VOCAB`) is set to 128 ASCII characters. The values below set for `HIDDEN_SIZE`, `N_LAYERS` of the network, and the `LR` (Learning Rate) give an optimum balance for the network size and performance. They might need be set via experiments when working with other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.001\n",
    "N_LAYERS = 3\n",
    "CHAR_VOCAB = 128\n",
    "HIDDEN_SIZE = 100\n",
    "N_DOMAIN_TYPE = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate DGA Detector\n",
    "Now that the data is ready, the datasets are created, and we've set the parameters for the model, we can use the DGADetector method built into CLX to create and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = DGADetector(lr=LR)\n",
    "dd.init_model(n_layers=N_LAYERS, char_vocab=CHAR_VOCAB, hidden_size=HIDDEN_SIZE, n_domain_type=N_DOMAIN_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(dir_path):\n",
    "    print(\"Verify if directory `%s` is already exists.\" % (dir_path))\n",
    "    if not os.path.exists(dir_path):\n",
    "        print(\"Directory `%s` does not exists.\" % (dir_path))\n",
    "        print(\"Creating directory `%s` to store trained models.\" % (dir_path))\n",
    "        os.makedirs(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_cache():\n",
    "    # release memory.\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(dd, train_dataloader, test_dataloader, epoch, model_dir):\n",
    "    print(\"Initiating model training\")\n",
    "    create_dir(model_dir)\n",
    "    max_accuracy = 0\n",
    "    prev_model_file_path = \"\"\n",
    "    for i in range(1, epoch + 1):\n",
    "        print(\"---------\")\n",
    "        print(\"Epoch: %s\" % (i))\n",
    "        print(\"---------\")\n",
    "        dd.train_model(train_dataloader)\n",
    "        accuracy = dd.evaluate_model(test_dataloader)\n",
    "        now = datetime.now()\n",
    "        output_filepath = (\n",
    "            model_dir\n",
    "            + \"/\"\n",
    "            + \"rnn_classifier_{}.pth\".format(now.strftime(\"%Y-%m-%d_%H_%M_%S\"))\n",
    "        )\n",
    "        if accuracy > max_accuracy:\n",
    "            dd.save_model(output_filepath)\n",
    "            max_accuracy = accuracy\n",
    "            if prev_model_file_path:\n",
    "                os.remove(prev_model_file_path)\n",
    "            prev_model_file_path = output_filepath\n",
    "    print(\"Model with highest accuracy (%s) is stored to location %s\" % (max_accuracy, prev_model_file_path))\n",
    "    return prev_model_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation\n",
    "Using the function we created above, we now train and evaluate the model.\n",
    "*NOTE: You may see warnings when you run the training due to a [bug in PyTorch](https://github.com/pytorch/pytorch/issues/27972) which is being actively investigated.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating model training\n",
      "Verify if directory `/trained_models` is already exists.\n",
      "---------\n",
      "Epoch: 1\n",
      "---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.7/site-packages/cudf/io/dlpack.py:74: UserWarning: WARNING: cuDF to_dlpack() produces column-major (Fortran order) output. If the output tensor needs to be row major, transpose the output of this function.\n",
      "  return libdlpack.to_dlpack(gdf_cols)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100000/1433083 (7%)]\tLoss: 6934.00\n",
      "[200000/1433083 (14%)]\tLoss: 5124.41\n",
      "[300000/1433083 (21%)]\tLoss: 4003.11\n",
      "[400000/1433083 (28%)]\tLoss: 3448.60\n",
      "[500000/1433083 (35%)]\tLoss: 3041.25\n",
      "[600000/1433083 (42%)]\tLoss: 2624.86\n",
      "[700000/1433083 (49%)]\tLoss: 2311.83\n",
      "[800000/1433083 (56%)]\tLoss: 2062.95\n",
      "[900000/1433083 (63%)]\tLoss: 2269.48\n",
      "[1000000/1433083 (70%)]\tLoss: 2183.30\n",
      "[1100000/1433083 (77%)]\tLoss: 2202.50\n",
      "[1200000/1433083 (84%)]\tLoss: 2132.69\n",
      "[1300000/1433083 (91%)]\tLoss: 2126.96\n",
      "[1400000/1433083 (98%)]\tLoss: 2160.02\n",
      "Test set: Accuracy: 395451/614179 (0.643869295433416)\n",
      "\n",
      "---------\n",
      "Epoch: 2\n",
      "---------\n",
      "[100000/1433083 (7%)]\tLoss: 3572.17\n",
      "[200000/1433083 (14%)]\tLoss: 2562.24\n",
      "[300000/1433083 (21%)]\tLoss: 1891.83\n",
      "[400000/1433083 (28%)]\tLoss: 1623.27\n",
      "[500000/1433083 (35%)]\tLoss: 1427.64\n",
      "[600000/1433083 (42%)]\tLoss: 1224.27\n",
      "[700000/1433083 (49%)]\tLoss: 1073.77\n",
      "[800000/1433083 (56%)]\tLoss: 957.13\n",
      "[900000/1433083 (63%)]\tLoss: 920.59\n",
      "[1000000/1433083 (70%)]\tLoss: 917.27\n",
      "[1100000/1433083 (77%)]\tLoss: 944.63\n",
      "[1200000/1433083 (84%)]\tLoss: 939.01\n",
      "[1300000/1433083 (91%)]\tLoss: 984.52\n",
      "[1400000/1433083 (98%)]\tLoss: 1079.38\n",
      "Test set: Accuracy: 539359/614179 (0.8781788371142615)\n",
      "\n",
      "---------\n",
      "Epoch: 3\n",
      "---------\n",
      "[100000/1433083 (7%)]\tLoss: 1530.68\n",
      "[200000/1433083 (14%)]\tLoss: 1150.24\n",
      "[300000/1433083 (21%)]\tLoss: 880.55\n",
      "[400000/1433083 (28%)]\tLoss: 771.39\n",
      "[500000/1433083 (35%)]\tLoss: 700.58\n",
      "[600000/1433083 (42%)]\tLoss: 603.65\n",
      "[700000/1433083 (49%)]\tLoss: 531.96\n",
      "[800000/1433083 (56%)]\tLoss: 476.47\n",
      "[900000/1433083 (63%)]\tLoss: 481.26\n",
      "[1000000/1433083 (70%)]\tLoss: 506.48\n",
      "[1100000/1433083 (77%)]\tLoss: 515.28\n",
      "[1200000/1433083 (84%)]\tLoss: 536.77\n",
      "[1300000/1433083 (91%)]\tLoss: 604.75\n",
      "[1400000/1433083 (98%)]\tLoss: 715.25\n",
      "Test set: Accuracy: 582241/614179 (0.9479988732926394)\n",
      "\n",
      "---------\n",
      "Epoch: 4\n",
      "---------\n",
      "[100000/1433083 (7%)]\tLoss: 1258.25\n",
      "[200000/1433083 (14%)]\tLoss: 901.03\n",
      "[300000/1433083 (21%)]\tLoss: 698.06\n",
      "[400000/1433083 (28%)]\tLoss: 606.89\n",
      "[500000/1433083 (35%)]\tLoss: 553.34\n",
      "[600000/1433083 (42%)]\tLoss: 477.87\n",
      "[700000/1433083 (49%)]\tLoss: 421.97\n",
      "[800000/1433083 (56%)]\tLoss: 378.73\n",
      "[900000/1433083 (63%)]\tLoss: 389.13\n",
      "[1000000/1433083 (70%)]\tLoss: 419.55\n",
      "[1100000/1433083 (77%)]\tLoss: 423.60\n",
      "[1200000/1433083 (84%)]\tLoss: 449.53\n",
      "[1300000/1433083 (91%)]\tLoss: 500.13\n",
      "[1400000/1433083 (98%)]\tLoss: 615.82\n",
      "Test set: Accuracy: 593620/614179 (0.9665260453385739)\n",
      "\n",
      "---------\n",
      "Epoch: 5\n",
      "---------\n",
      "[100000/1433083 (7%)]\tLoss: 461.49\n",
      "[200000/1433083 (14%)]\tLoss: 432.40\n",
      "[300000/1433083 (21%)]\tLoss: 366.09\n",
      "[400000/1433083 (28%)]\tLoss: 337.85\n",
      "[500000/1433083 (35%)]\tLoss: 324.12\n",
      "[600000/1433083 (42%)]\tLoss: 282.67\n",
      "[700000/1433083 (49%)]\tLoss: 252.11\n",
      "[800000/1433083 (56%)]\tLoss: 228.26\n",
      "[900000/1433083 (63%)]\tLoss: 246.65\n",
      "[1000000/1433083 (70%)]\tLoss: 281.92\n",
      "[1100000/1433083 (77%)]\tLoss: 288.35\n",
      "[1200000/1433083 (84%)]\tLoss: 320.46\n",
      "[1300000/1433083 (91%)]\tLoss: 384.15\n",
      "[1400000/1433083 (98%)]\tLoss: 501.97\n",
      "Test set: Accuracy: 594871/614179 (0.9685629108126458)\n",
      "\n",
      "---------\n",
      "Epoch: 6\n",
      "---------\n",
      "[100000/1433083 (7%)]\tLoss: 228.09\n",
      "[200000/1433083 (14%)]\tLoss: 252.22\n",
      "[300000/1433083 (21%)]\tLoss: 230.91\n",
      "[400000/1433083 (28%)]\tLoss: 220.80\n",
      "[500000/1433083 (35%)]\tLoss: 222.13\n",
      "[600000/1433083 (42%)]\tLoss: 195.36\n",
      "[700000/1433083 (49%)]\tLoss: 175.94\n",
      "[800000/1433083 (56%)]\tLoss: 160.43\n",
      "[900000/1433083 (63%)]\tLoss: 179.98\n",
      "[1000000/1433083 (70%)]\tLoss: 230.58\n",
      "[1100000/1433083 (77%)]\tLoss: 238.54\n",
      "[1200000/1433083 (84%)]\tLoss: 266.56\n",
      "[1300000/1433083 (91%)]\tLoss: 312.42\n",
      "[1400000/1433083 (98%)]\tLoss: 429.01\n",
      "Test set: Accuracy: 595502/614179 (0.9695902985937325)\n",
      "\n",
      "---------\n",
      "Epoch: 7\n",
      "---------\n",
      "[100000/1433083 (7%)]\tLoss: 205.40\n",
      "[200000/1433083 (14%)]\tLoss: 206.40\n",
      "[300000/1433083 (21%)]\tLoss: 190.54\n",
      "[400000/1433083 (28%)]\tLoss: 182.81\n",
      "[500000/1433083 (35%)]\tLoss: 185.79\n",
      "[600000/1433083 (42%)]\tLoss: 163.40\n",
      "[700000/1433083 (49%)]\tLoss: 147.28\n",
      "[800000/1433083 (56%)]\tLoss: 134.35\n",
      "[900000/1433083 (63%)]\tLoss: 151.73\n",
      "[1000000/1433083 (70%)]\tLoss: 207.27\n",
      "[1100000/1433083 (77%)]\tLoss: 215.37\n",
      "[1200000/1433083 (84%)]\tLoss: 242.73\n",
      "[1300000/1433083 (91%)]\tLoss: 281.62\n",
      "[1400000/1433083 (98%)]\tLoss: 391.87\n",
      "Test set: Accuracy: 597311/614179 (0.9725356939914911)\n",
      "\n",
      "---------\n",
      "Epoch: 8\n",
      "---------\n",
      "[100000/1433083 (7%)]\tLoss: 187.40\n",
      "[200000/1433083 (14%)]\tLoss: 179.40\n",
      "[300000/1433083 (21%)]\tLoss: 163.87\n",
      "[400000/1433083 (28%)]\tLoss: 156.45\n",
      "[500000/1433083 (35%)]\tLoss: 159.50\n",
      "[600000/1433083 (42%)]\tLoss: 140.26\n",
      "[700000/1433083 (49%)]\tLoss: 126.28\n",
      "[800000/1433083 (56%)]\tLoss: 114.96\n",
      "[900000/1433083 (63%)]\tLoss: 129.74\n",
      "[1000000/1433083 (70%)]\tLoss: 164.02\n",
      "[1100000/1433083 (77%)]\tLoss: 169.85\n",
      "[1200000/1433083 (84%)]\tLoss: 196.02\n",
      "[1300000/1433083 (91%)]\tLoss: 242.76\n",
      "[1400000/1433083 (98%)]\tLoss: 349.33\n",
      "Test set: Accuracy: 599865/614179 (0.9766940908106594)\n",
      "\n",
      "---------\n",
      "Epoch: 9\n",
      "---------\n",
      "[100000/1433083 (7%)]\tLoss: 191.84\n",
      "[200000/1433083 (14%)]\tLoss: 170.52\n",
      "[300000/1433083 (21%)]\tLoss: 151.75\n",
      "[400000/1433083 (28%)]\tLoss: 141.89\n",
      "[500000/1433083 (35%)]\tLoss: 142.50\n",
      "[600000/1433083 (42%)]\tLoss: 125.61\n",
      "[700000/1433083 (49%)]\tLoss: 113.02\n",
      "[800000/1433083 (56%)]\tLoss: 102.84\n",
      "[900000/1433083 (63%)]\tLoss: 115.77\n",
      "[1000000/1433083 (70%)]\tLoss: 152.47\n",
      "[1100000/1433083 (77%)]\tLoss: 157.99\n",
      "[1200000/1433083 (84%)]\tLoss: 180.06\n",
      "[1300000/1433083 (91%)]\tLoss: 212.72\n",
      "[1400000/1433083 (98%)]\tLoss: 315.54\n",
      "Test set: Accuracy: 598551/614179 (0.9745546493774616)\n",
      "\n",
      "---------\n",
      "Epoch: 10\n",
      "---------\n",
      "[100000/1433083 (7%)]\tLoss: 181.71\n",
      "[200000/1433083 (14%)]\tLoss: 152.54\n",
      "[300000/1433083 (21%)]\tLoss: 133.42\n",
      "[400000/1433083 (28%)]\tLoss: 123.68\n",
      "[500000/1433083 (35%)]\tLoss: 124.12\n",
      "[600000/1433083 (42%)]\tLoss: 109.35\n",
      "[700000/1433083 (49%)]\tLoss: 98.38\n",
      "[800000/1433083 (56%)]\tLoss: 89.48\n",
      "[900000/1433083 (63%)]\tLoss: 101.28\n",
      "[1000000/1433083 (70%)]\tLoss: 135.53\n",
      "[1100000/1433083 (77%)]\tLoss: 140.14\n",
      "[1200000/1433083 (84%)]\tLoss: 160.20\n",
      "[1300000/1433083 (91%)]\tLoss: 192.95\n",
      "[1400000/1433083 (98%)]\tLoss: 291.04\n",
      "Test set: Accuracy: 602861/614179 (0.9815721475335366)\n",
      "\n",
      "---------\n",
      "Epoch: 11\n",
      "---------\n",
      "[100000/1433083 (7%)]\tLoss: 161.41\n",
      "[200000/1433083 (14%)]\tLoss: 135.35\n",
      "[300000/1433083 (21%)]\tLoss: 119.28\n",
      "[400000/1433083 (28%)]\tLoss: 109.76\n",
      "[500000/1433083 (35%)]\tLoss: 111.91\n",
      "[600000/1433083 (42%)]\tLoss: 98.85\n",
      "[700000/1433083 (49%)]\tLoss: 88.98\n",
      "[800000/1433083 (56%)]\tLoss: 80.94\n",
      "[900000/1433083 (63%)]\tLoss: 91.54\n",
      "[1000000/1433083 (70%)]\tLoss: 124.67\n",
      "[1100000/1433083 (77%)]\tLoss: 128.81\n",
      "[1200000/1433083 (84%)]\tLoss: 146.43\n",
      "[1300000/1433083 (91%)]\tLoss: 175.28\n",
      "[1400000/1433083 (98%)]\tLoss: 268.99\n",
      "Test set: Accuracy: 603683/614179 (0.9829105195716559)\n",
      "\n",
      "---------\n",
      "Epoch: 12\n",
      "---------\n",
      "[100000/1433083 (7%)]\tLoss: 141.27\n",
      "[200000/1433083 (14%)]\tLoss: 117.43\n",
      "[300000/1433083 (21%)]\tLoss: 103.08\n",
      "[400000/1433083 (28%)]\tLoss: 94.98\n",
      "[500000/1433083 (35%)]\tLoss: 102.36\n",
      "[600000/1433083 (42%)]\tLoss: 92.50\n",
      "[700000/1433083 (49%)]\tLoss: 85.16\n",
      "[800000/1433083 (56%)]\tLoss: 78.46\n",
      "[900000/1433083 (63%)]\tLoss: 90.17\n",
      "[1000000/1433083 (70%)]\tLoss: 117.16\n",
      "[1100000/1433083 (77%)]\tLoss: 120.08\n",
      "[1200000/1433083 (84%)]\tLoss: 135.48\n",
      "[1300000/1433083 (91%)]\tLoss: 161.99\n",
      "[1400000/1433083 (98%)]\tLoss: 248.56\n",
      "Test set: Accuracy: 604999/614179 (0.9850532173845085)\n",
      "\n",
      "---------\n",
      "Epoch: 13\n",
      "---------\n",
      "[100000/1433083 (7%)]\tLoss: 125.53\n",
      "[200000/1433083 (14%)]\tLoss: 105.53\n",
      "[300000/1433083 (21%)]\tLoss: 92.27\n",
      "[400000/1433083 (28%)]\tLoss: 84.73\n",
      "[500000/1433083 (35%)]\tLoss: 86.02\n",
      "[600000/1433083 (42%)]\tLoss: 76.03\n",
      "[700000/1433083 (49%)]\tLoss: 68.71\n",
      "[800000/1433083 (56%)]\tLoss: 62.57\n",
      "[900000/1433083 (63%)]\tLoss: 72.19\n",
      "[1000000/1433083 (70%)]\tLoss: 89.43\n",
      "[1100000/1433083 (77%)]\tLoss: 92.67\n",
      "[1200000/1433083 (84%)]\tLoss: 110.61\n",
      "[1300000/1433083 (91%)]\tLoss: 135.45\n",
      "[1400000/1433083 (98%)]\tLoss: 223.15\n",
      "Test set: Accuracy: 605434/614179 (0.9857614799594255)\n",
      "\n",
      "---------\n",
      "Epoch: 14\n",
      "---------\n",
      "[100000/1433083 (7%)]\tLoss: 114.11\n",
      "[200000/1433083 (14%)]\tLoss: 92.70\n",
      "[300000/1433083 (21%)]\tLoss: 81.29\n",
      "[400000/1433083 (28%)]\tLoss: 74.57\n",
      "[500000/1433083 (35%)]\tLoss: 78.70\n",
      "[600000/1433083 (42%)]\tLoss: 69.54\n",
      "[700000/1433083 (49%)]\tLoss: 62.85\n",
      "[800000/1433083 (56%)]\tLoss: 57.12\n",
      "[900000/1433083 (63%)]\tLoss: 65.25\n",
      "[1000000/1433083 (70%)]\tLoss: 102.05\n",
      "[1100000/1433083 (77%)]\tLoss: 104.56\n",
      "[1200000/1433083 (84%)]\tLoss: 116.81\n",
      "[1300000/1433083 (91%)]\tLoss: 136.89\n",
      "[1400000/1433083 (98%)]\tLoss: 211.69\n",
      "Test set: Accuracy: 606473/614179 (0.9874531691900895)\n",
      "\n",
      "---------\n",
      "Epoch: 15\n",
      "---------\n",
      "[100000/1433083 (7%)]\tLoss: 109.01\n",
      "[200000/1433083 (14%)]\tLoss: 434.04\n",
      "[300000/1433083 (21%)]\tLoss: 368.20\n",
      "[400000/1433083 (28%)]\tLoss: 312.78\n",
      "[500000/1433083 (35%)]\tLoss: 274.18\n",
      "[600000/1433083 (42%)]\tLoss: 234.16\n",
      "[700000/1433083 (49%)]\tLoss: 205.38\n",
      "[800000/1433083 (56%)]\tLoss: 183.20\n",
      "[900000/1433083 (63%)]\tLoss: 180.85\n",
      "[1000000/1433083 (70%)]\tLoss: 188.61\n",
      "[1100000/1433083 (77%)]\tLoss: 184.21\n",
      "[1200000/1433083 (84%)]\tLoss: 191.00\n",
      "[1300000/1433083 (91%)]\tLoss: 204.63\n",
      "[1400000/1433083 (98%)]\tLoss: 267.06\n",
      "Test set: Accuracy: 607072/614179 (0.9884284548966995)\n",
      "\n",
      "---------\n",
      "Epoch: 16\n",
      "---------\n",
      "[100000/1433083 (7%)]\tLoss: 132.83\n",
      "[200000/1433083 (14%)]\tLoss: 102.21\n",
      "[300000/1433083 (21%)]\tLoss: 87.72\n",
      "[400000/1433083 (28%)]\tLoss: 79.24\n",
      "[500000/1433083 (35%)]\tLoss: 81.86\n",
      "[600000/1433083 (42%)]\tLoss: 72.21\n",
      "[700000/1433083 (49%)]\tLoss: 65.39\n",
      "[800000/1433083 (56%)]\tLoss: 59.41\n",
      "[900000/1433083 (63%)]\tLoss: 66.27\n",
      "[1000000/1433083 (70%)]\tLoss: 80.38\n",
      "[1100000/1433083 (77%)]\tLoss: 82.61\n",
      "[1200000/1433083 (84%)]\tLoss: 93.61\n",
      "[1300000/1433083 (91%)]\tLoss: 114.32\n",
      "[1400000/1433083 (98%)]\tLoss: 187.35\n",
      "Test set: Accuracy: 607740/614179 (0.9895160857013997)\n",
      "\n",
      "---------\n",
      "Epoch: 17\n",
      "---------\n",
      "[100000/1433083 (7%)]\tLoss: 88.05\n",
      "[200000/1433083 (14%)]\tLoss: 70.05\n",
      "[300000/1433083 (21%)]\tLoss: 62.35\n",
      "[400000/1433083 (28%)]\tLoss: 57.83\n",
      "[500000/1433083 (35%)]\tLoss: 69.02\n",
      "[600000/1433083 (42%)]\tLoss: 61.06\n",
      "[700000/1433083 (49%)]\tLoss: 55.42\n",
      "[800000/1433083 (56%)]\tLoss: 50.37\n",
      "[900000/1433083 (63%)]\tLoss: 56.73\n",
      "[1000000/1433083 (70%)]\tLoss: 69.44\n",
      "[1100000/1433083 (77%)]\tLoss: 71.47\n",
      "[1200000/1433083 (84%)]\tLoss: 81.53\n",
      "[1300000/1433083 (91%)]\tLoss: 105.73\n",
      "[1400000/1433083 (98%)]\tLoss: 180.73\n",
      "Test set: Accuracy: 607579/614179 (0.989253947139189)\n",
      "\n",
      "---------\n",
      "Epoch: 18\n",
      "---------\n",
      "[100000/1433083 (7%)]\tLoss: 84.88\n",
      "[200000/1433083 (14%)]\tLoss: 65.50\n",
      "[300000/1433083 (21%)]\tLoss: 58.56\n",
      "[400000/1433083 (28%)]\tLoss: 53.17\n",
      "[500000/1433083 (35%)]\tLoss: 61.10\n",
      "[600000/1433083 (42%)]\tLoss: 56.51\n",
      "[700000/1433083 (49%)]\tLoss: 52.31\n",
      "[800000/1433083 (56%)]\tLoss: 47.77\n",
      "[900000/1433083 (63%)]\tLoss: 53.87\n",
      "[1000000/1433083 (70%)]\tLoss: 64.68\n",
      "[1100000/1433083 (77%)]\tLoss: 66.14\n",
      "[1200000/1433083 (84%)]\tLoss: 75.23\n",
      "[1300000/1433083 (91%)]\tLoss: 90.18\n",
      "[1400000/1433083 (98%)]\tLoss: 181.09\n",
      "Test set: Accuracy: 605978/614179 (0.9866472152255287)\n",
      "\n",
      "---------\n",
      "Epoch: 19\n",
      "---------\n",
      "[100000/1433083 (7%)]\tLoss: 72.08\n",
      "[200000/1433083 (14%)]\tLoss: 58.86\n",
      "[300000/1433083 (21%)]\tLoss: 51.84\n",
      "[400000/1433083 (28%)]\tLoss: 47.63\n",
      "[500000/1433083 (35%)]\tLoss: 52.17\n",
      "[600000/1433083 (42%)]\tLoss: 46.40\n",
      "[700000/1433083 (49%)]\tLoss: 42.40\n",
      "[800000/1433083 (56%)]\tLoss: 38.56\n",
      "[900000/1433083 (63%)]\tLoss: 44.67\n",
      "[1000000/1433083 (70%)]\tLoss: 56.93\n",
      "[1100000/1433083 (77%)]\tLoss: 58.61\n",
      "[1200000/1433083 (84%)]\tLoss: 67.20\n",
      "[1300000/1433083 (91%)]\tLoss: 83.15\n",
      "[1400000/1433083 (98%)]\tLoss: 152.65\n",
      "Test set: Accuracy: 607989/614179 (0.9899215049684212)\n",
      "\n",
      "---------\n",
      "Epoch: 20\n",
      "---------\n",
      "[100000/1433083 (7%)]\tLoss: 71.81\n",
      "[200000/1433083 (14%)]\tLoss: 56.67\n",
      "[300000/1433083 (21%)]\tLoss: 50.44\n",
      "[400000/1433083 (28%)]\tLoss: 45.07\n",
      "[500000/1433083 (35%)]\tLoss: 55.41\n",
      "[600000/1433083 (42%)]\tLoss: 50.76\n",
      "[700000/1433083 (49%)]\tLoss: 46.60\n",
      "[800000/1433083 (56%)]\tLoss: 42.36\n",
      "[900000/1433083 (63%)]\tLoss: 47.38\n",
      "[1000000/1433083 (70%)]\tLoss: 56.12\n",
      "[1100000/1433083 (77%)]\tLoss: 56.90\n",
      "[1200000/1433083 (84%)]\tLoss: 64.91\n",
      "[1300000/1433083 (91%)]\tLoss: 77.47\n",
      "[1400000/1433083 (98%)]\tLoss: 149.21\n",
      "Test set: Accuracy: 607598/614179 (0.9892848827459095)\n",
      "\n",
      "---------\n",
      "Epoch: 21\n",
      "---------\n",
      "[100000/1433083 (7%)]\tLoss: 113.23\n",
      "[200000/1433083 (14%)]\tLoss: 74.90\n",
      "[300000/1433083 (21%)]\tLoss: 60.07\n",
      "[400000/1433083 (28%)]\tLoss: 51.98\n",
      "[500000/1433083 (35%)]\tLoss: 56.12\n",
      "[600000/1433083 (42%)]\tLoss: 51.55\n",
      "[700000/1433083 (49%)]\tLoss: 47.20\n",
      "[800000/1433083 (56%)]\tLoss: 42.70\n",
      "[900000/1433083 (63%)]\tLoss: 46.98\n",
      "[1000000/1433083 (70%)]\tLoss: 56.60\n",
      "[1100000/1433083 (77%)]\tLoss: 58.90\n",
      "[1200000/1433083 (84%)]\tLoss: 67.79\n",
      "[1300000/1433083 (91%)]\tLoss: 90.08\n",
      "[1400000/1433083 (98%)]\tLoss: 156.07\n",
      "Test set: Accuracy: 607833/614179 (0.9896675073553476)\n",
      "\n",
      "---------\n",
      "Epoch: 22\n",
      "---------\n",
      "[100000/1433083 (7%)]\tLoss: 78.88\n",
      "[200000/1433083 (14%)]\tLoss: 58.00\n",
      "[300000/1433083 (21%)]\tLoss: 49.42\n",
      "[400000/1433083 (28%)]\tLoss: 42.90\n",
      "[500000/1433083 (35%)]\tLoss: 46.43\n",
      "[600000/1433083 (42%)]\tLoss: 41.18\n",
      "[700000/1433083 (49%)]\tLoss: 37.38\n",
      "[800000/1433083 (56%)]\tLoss: 33.79\n",
      "[900000/1433083 (63%)]\tLoss: 37.87\n",
      "[1000000/1433083 (70%)]\tLoss: 45.52\n",
      "[1100000/1433083 (77%)]\tLoss: 46.95\n",
      "[1200000/1433083 (84%)]\tLoss: 52.87\n",
      "[1300000/1433083 (91%)]\tLoss: 65.18\n",
      "[1400000/1433083 (98%)]\tLoss: 128.28\n",
      "Test set: Accuracy: 608334/614179 (0.9904832304588728)\n",
      "\n",
      "---------\n",
      "Epoch: 23\n",
      "---------\n",
      "[100000/1433083 (7%)]\tLoss: 196.00\n",
      "[200000/1433083 (14%)]\tLoss: 122.19\n",
      "[300000/1433083 (21%)]\tLoss: 98.86\n",
      "[400000/1433083 (28%)]\tLoss: 100.91\n",
      "[500000/1433083 (35%)]\tLoss: 93.13\n",
      "[600000/1433083 (42%)]\tLoss: 80.91\n",
      "[700000/1433083 (49%)]\tLoss: 72.03\n",
      "[800000/1433083 (56%)]\tLoss: 64.65\n",
      "[900000/1433083 (63%)]\tLoss: 66.81\n",
      "[1000000/1433083 (70%)]\tLoss: 74.25\n",
      "[1100000/1433083 (77%)]\tLoss: 73.06\n",
      "[1200000/1433083 (84%)]\tLoss: 77.63\n",
      "[1300000/1433083 (91%)]\tLoss: 86.10\n",
      "[1400000/1433083 (98%)]\tLoss: 139.90\n",
      "Test set: Accuracy: 608549/614179 (0.9908332912717628)\n",
      "\n",
      "---------\n",
      "Epoch: 24\n",
      "---------\n",
      "[100000/1433083 (7%)]\tLoss: 66.91\n",
      "[200000/1433083 (14%)]\tLoss: 48.34\n",
      "[300000/1433083 (21%)]\tLoss: 41.01\n",
      "[400000/1433083 (28%)]\tLoss: 36.24\n",
      "[500000/1433083 (35%)]\tLoss: 37.24\n",
      "[600000/1433083 (42%)]\tLoss: 33.11\n",
      "[700000/1433083 (49%)]\tLoss: 30.36\n",
      "[800000/1433083 (56%)]\tLoss: 27.48\n",
      "[900000/1433083 (63%)]\tLoss: 34.66\n",
      "[1000000/1433083 (70%)]\tLoss: 47.82\n",
      "[1100000/1433083 (77%)]\tLoss: 47.66\n",
      "[1200000/1433083 (84%)]\tLoss: 52.33\n",
      "[1300000/1433083 (91%)]\tLoss: 60.67\n",
      "[1400000/1433083 (98%)]\tLoss: 128.34\n",
      "Test set: Accuracy: 608036/614179 (0.9899980298903088)\n",
      "\n",
      "---------\n",
      "Epoch: 25\n",
      "---------\n",
      "[100000/1433083 (7%)]\tLoss: 46.52\n",
      "[200000/1433083 (14%)]\tLoss: 34.99\n",
      "[300000/1433083 (21%)]\tLoss: 30.30\n",
      "[400000/1433083 (28%)]\tLoss: 26.85\n",
      "[500000/1433083 (35%)]\tLoss: 28.50\n",
      "[600000/1433083 (42%)]\tLoss: 25.56\n",
      "[700000/1433083 (49%)]\tLoss: 23.64\n",
      "[800000/1433083 (56%)]\tLoss: 21.47\n",
      "[900000/1433083 (63%)]\tLoss: 27.64\n",
      "[1000000/1433083 (70%)]\tLoss: 34.46\n",
      "[1100000/1433083 (77%)]\tLoss: 34.91\n",
      "[1200000/1433083 (84%)]\tLoss: 39.49\n",
      "[1300000/1433083 (91%)]\tLoss: 47.69\n",
      "[1400000/1433083 (98%)]\tLoss: 111.56\n",
      "Test set: Accuracy: 608509/614179 (0.9907681636786669)\n",
      "\n",
      "---------\n",
      "Epoch: 26\n",
      "---------\n",
      "[100000/1433083 (7%)]\tLoss: 40.75\n",
      "[200000/1433083 (14%)]\tLoss: 30.45\n",
      "[300000/1433083 (21%)]\tLoss: 26.32\n",
      "[400000/1433083 (28%)]\tLoss: 23.22\n",
      "[500000/1433083 (35%)]\tLoss: 26.12\n",
      "[600000/1433083 (42%)]\tLoss: 23.39\n",
      "[700000/1433083 (49%)]\tLoss: 21.57\n",
      "[800000/1433083 (56%)]\tLoss: 19.57\n",
      "[900000/1433083 (63%)]\tLoss: 22.94\n",
      "[1000000/1433083 (70%)]\tLoss: 32.46\n",
      "[1100000/1433083 (77%)]\tLoss: 32.70\n",
      "[1200000/1433083 (84%)]\tLoss: 36.70\n",
      "[1300000/1433083 (91%)]\tLoss: 43.35\n",
      "[1400000/1433083 (98%)]\tLoss: 91.07\n",
      "Test set: Accuracy: 608565/614179 (0.9908593423090011)\n",
      "\n",
      "---------\n",
      "Epoch: 27\n",
      "---------\n",
      "[100000/1433083 (7%)]\tLoss: 450.62\n",
      "[200000/1433083 (14%)]\tLoss: 280.06\n",
      "[300000/1433083 (21%)]\tLoss: 208.37\n",
      "[400000/1433083 (28%)]\tLoss: 169.89\n",
      "[500000/1433083 (35%)]\tLoss: 151.04\n",
      "[600000/1433083 (42%)]\tLoss: 129.00\n",
      "[700000/1433083 (49%)]\tLoss: 113.50\n",
      "[800000/1433083 (56%)]\tLoss: 101.17\n",
      "[900000/1433083 (63%)]\tLoss: 100.15\n",
      "[1000000/1433083 (70%)]\tLoss: 103.96\n",
      "[1100000/1433083 (77%)]\tLoss: 100.03\n",
      "[1200000/1433083 (84%)]\tLoss: 101.32\n",
      "[1300000/1433083 (91%)]\tLoss: 105.45\n",
      "[1400000/1433083 (98%)]\tLoss: 149.18\n",
      "Test set: Accuracy: 608366/614179 (0.9905353325333494)\n",
      "\n",
      "---------\n",
      "Epoch: 28\n",
      "---------\n",
      "[100000/1433083 (7%)]\tLoss: 51.12\n",
      "[200000/1433083 (14%)]\tLoss: 37.79\n",
      "[300000/1433083 (21%)]\tLoss: 31.71\n",
      "[400000/1433083 (28%)]\tLoss: 28.33\n",
      "[500000/1433083 (35%)]\tLoss: 29.14\n",
      "[600000/1433083 (42%)]\tLoss: 26.01\n",
      "[700000/1433083 (49%)]\tLoss: 23.98\n",
      "[800000/1433083 (56%)]\tLoss: 21.73\n",
      "[900000/1433083 (63%)]\tLoss: 24.66\n",
      "[1000000/1433083 (70%)]\tLoss: 31.09\n",
      "[1100000/1433083 (77%)]\tLoss: 31.48\n",
      "[1200000/1433083 (84%)]\tLoss: 35.43\n",
      "[1300000/1433083 (91%)]\tLoss: 41.33\n",
      "[1400000/1433083 (98%)]\tLoss: 88.57\n",
      "Test set: Accuracy: 609081/614179 (0.9916994882599373)\n",
      "\n",
      "---------\n",
      "Epoch: 29\n",
      "---------\n",
      "[100000/1433083 (7%)]\tLoss: 34.29\n",
      "[200000/1433083 (14%)]\tLoss: 26.16\n",
      "[300000/1433083 (21%)]\tLoss: 22.32\n",
      "[400000/1433083 (28%)]\tLoss: 19.52\n",
      "[500000/1433083 (35%)]\tLoss: 34.13\n",
      "[600000/1433083 (42%)]\tLoss: 34.60\n",
      "[700000/1433083 (49%)]\tLoss: 34.53\n",
      "[800000/1433083 (56%)]\tLoss: 32.85\n",
      "[900000/1433083 (63%)]\tLoss: 38.88\n",
      "[1000000/1433083 (70%)]\tLoss: 43.05\n",
      "[1100000/1433083 (77%)]\tLoss: 41.89\n",
      "[1200000/1433083 (84%)]\tLoss: 43.97\n",
      "[1300000/1433083 (91%)]\tLoss: 48.01\n",
      "[1400000/1433083 (98%)]\tLoss: 99.48\n",
      "Test set: Accuracy: 608744/614179 (0.9911507882881049)\n",
      "\n",
      "---------\n",
      "Epoch: 30\n",
      "---------\n",
      "[100000/1433083 (7%)]\tLoss: 34.00\n",
      "[200000/1433083 (14%)]\tLoss: 26.24\n",
      "[300000/1433083 (21%)]\tLoss: 22.55\n",
      "[400000/1433083 (28%)]\tLoss: 19.48\n",
      "[500000/1433083 (35%)]\tLoss: 20.57\n",
      "[600000/1433083 (42%)]\tLoss: 18.53\n",
      "[700000/1433083 (49%)]\tLoss: 17.22\n",
      "[800000/1433083 (56%)]\tLoss: 15.63\n",
      "[900000/1433083 (63%)]\tLoss: 18.12\n",
      "[1000000/1433083 (70%)]\tLoss: 22.57\n",
      "[1100000/1433083 (77%)]\tLoss: 22.66\n",
      "[1200000/1433083 (84%)]\tLoss: 25.37\n",
      "[1300000/1433083 (91%)]\tLoss: 29.47\n",
      "[1400000/1433083 (98%)]\tLoss: 85.57\n",
      "Test set: Accuracy: 608305/614179 (0.9904360129538783)\n",
      "\n",
      "Model with highest accuracy (0.9916994882599373) is stored to location /trained_models/rnn_classifier_2020-12-17_20_58_13.pth\n",
      "CPU times: user 11h 54min 19s, sys: 1h 55min 7s, total: 13h 49min 26s\n",
      "Wall time: 11min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epoch = 30\n",
    "model_dir='/trained_models'\n",
    "model_filepath = train_and_eval(dd, train_dataloader, test_dataloader, epoch, model_dir)\n",
    "cleanup_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the model generated above, we now score the test dataset against the model to determine if the domain is likely generated by a DGA or benign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 0.9916994882599373\n"
     ]
    }
   ],
   "source": [
    "dd = DGADetector()\n",
    "dd.load_model(model_filepath)\n",
    "\n",
    "pred_results = []\n",
    "true_results = []\n",
    "for chunk in test_dataloader.get_chunks():\n",
    "    pred_results.append(list(dd.predict(chunk['domain']).values_host))\n",
    "    true_results.append(list(chunk['type'].values_host))\n",
    "pred_results = np.concatenate(pred_results)\n",
    "true_results = np.concatenate(true_results)\n",
    "accuracy_score = accuracy_score(pred_results, true_results)\n",
    "print('Model accuracy: %s'%(accuracy_score))\n",
    "cleanup_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision score: 0.978\n"
     ]
    }
   ],
   "source": [
    "average_precision = average_precision_score(true_results, pred_results)\n",
    "\n",
    "print('Average precision score: {0:0.3f}'.format(average_precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DGA detector in CLX enables users to train their models for detection and also use existing models. This capability could also be used in conjunction with log parsing efforts if the logs contain domain names. DGA detection done with CLX and RAPIDS keeps data in GPU memory, removing unnecessary copy/converts and providing a 4X speed advantage over CPU only implementations. This is esepcially true with large batch sizes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
