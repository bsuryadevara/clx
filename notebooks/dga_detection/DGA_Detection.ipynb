{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain Generation Algorithm (DGA) Detection\n",
    "\n",
    "## Authors\n",
    " - Gorkem Batmaz (NVIDIA) [gbatmaz@nvidia.com]\n",
    " - Bhargav Suryadevara (NVIDIA) [bsuryadevara@nvidia.com]\n",
    "\n",
    "## Development Notes\n",
    "* Developed using: RAPIDS v0.12.0 and CLX v0.12\n",
    "* Last tested using: RAPIDS v0.12.0 and CLX v0.12 on Jan 28, 2020\n",
    "\n",
    "## Table of Contents\n",
    "* Introduction\n",
    "* Data Importing\n",
    "* Data Preprocessing\n",
    "* Training and Evaluation\n",
    "* Inference\n",
    "* Conclusion\n",
    "\n",
    "## Introduction\n",
    "[Domain Generation Algorithms](https://en.wikipedia.org/wiki/Domain_generation_algorithm) (DGAs) are used to generate domain names that can be used by the malware to communicate with the command and control servers. IP addresses and static domain names can be easily blocked, and a DGA provides an easy method to generate a large number of domain names and rotate through them to circumvent traditional block lists. We will use a type of recurrent neural network called the [Gated Recurrent Unit](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21) (GRU) for this example. The [CLX](https://github.com/rapidsai/clx) and [RAPIDS](https://rapids.ai) libraries enable users train their models with up-to-date domain names representative of both benign and DGA generated strings. Using a CLX workflow, this capability could also be used in production. This notebook provides a view into the data science workflow to create a DGA detection implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import cudf\n",
    "import torch\n",
    "import shutil\n",
    "import s3fs\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, average_precision_score\n",
    "from clx.analytics.detector_dataset import DetectorDataset\n",
    "from clx.analytics.dga_detector import DGADetector\n",
    "from clx.analytics import detector_utils as du\n",
    "from cuml.preprocessing.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_CSV = \"benign_and_dga_domains.csv\"\n",
    "\n",
    "S3_BASE_PATH = \"rapidsai-data/cyber/clx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Benign and DGA dataset\n",
    "if not os.path.exists(INPUT_CSV):\n",
    "    fs = s3fs.S3FileSystem(anon=True)\n",
    "    fs.get(S3_BASE_PATH + \"/\" + INPUT_CSV, INPUT_CSV)\n",
    "\n",
    "input_df = cudf.read_csv(INPUT_CSV)\n",
    "input_df = du.str2ascii(input_df, input_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Train and Test Dataset\n",
    "We utilize the [`train_test_split` function](https://docs.rapids.ai/api/cuml/0.10/api.html#model-selection-and-data-splitting) from [cuML](https://github.com/rapidsai/cuml) and create a shuffled dataset for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_train, domain_test, type_train, type_test = train_test_split(input_df, 'type', train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(domain_df, type_series):\n",
    "    df = cudf.DataFrame()\n",
    "    df['domain'] = domain_df['domain'].reset_index(drop=True)\n",
    "    df['type'] = type_series.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = create_df(domain_test, type_test)\n",
    "train_df = create_df(domain_train, type_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have only benign and DGA (malicious) categoriesm, the number of domain types need to be set to 2 (`N_DOMAIN_TYPE=2`). Vocabulary size(`CHAR_VOCAB`) is set to 128 ASCII characters. The values below set for `HIDDEN_SIZE`, `N_LAYERS` of the network, and the `LR` (Learning Rate) give an optimum balance for the network size and performance. They might need be set via experiments when working with other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.001\n",
    "N_LAYERS = 3\n",
    "CHAR_VOCAB = 128\n",
    "HIDDEN_SIZE = 100\n",
    "N_DOMAIN_TYPE = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate DGA Detector\n",
    "Now that the data is ready, the datasets are created, and we've set the parameters for the model, we can use the DGADetector method built into CLX to create and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = DGADetector(lr=LR)\n",
    "dd.init_model(n_layers=N_LAYERS, char_vocab=CHAR_VOCAB, hidden_size=HIDDEN_SIZE, n_domain_type=N_DOMAIN_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Batches\n",
    "We need to partition the input dataframe into one or more smaller dataframes per the given batch size for training and testing of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10000\n",
    "train_dataset = DetectorDataset(train_df, batchsize=batch_size)\n",
    "test_dataset = DetectorDataset(test_df, batchsize=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(dir_path):\n",
    "    print(\"Verify if directory `%s` is already exists.\" % (dir_path))\n",
    "    if not os.path.exists(dir_path):\n",
    "        print(\"Directory `%s` does not exists.\" % (dir_path))\n",
    "        print(\"Creating directory `%s` to store trained models.\" % (dir_path))\n",
    "        os.makedirs(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_cache():\n",
    "    # release memory.\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(dd, train_dataset, test_dataset, epoch, model_dir):\n",
    "    print(\"Initiating model training\")\n",
    "    create_dir(model_dir)\n",
    "    max_accuracy = 0\n",
    "    prev_model_file_path = \"\"\n",
    "    for i in range(1, epoch + 1):\n",
    "        print(\"---------\")\n",
    "        print(\"Epoch: %s\" % (i))\n",
    "        print(\"---------\")\n",
    "        dd.train_model(train_dataset)\n",
    "        accuracy = dd.evaluate_model(test_dataset)\n",
    "        now = datetime.now()\n",
    "        output_filepath = (\n",
    "            model_dir\n",
    "            + \"/\"\n",
    "            + \"rnn_classifier_{}.pth\".format(now.strftime(\"%Y-%m-%d_%H_%M_%S\"))\n",
    "        )\n",
    "        if accuracy > max_accuracy:\n",
    "            dd.save_model(output_filepath)\n",
    "            max_accuracy = accuracy\n",
    "            if prev_model_file_path:\n",
    "                os.remove(prev_model_file_path)\n",
    "            prev_model_file_path = output_filepath\n",
    "    print(\"Model with highest accuracy (%s) is stored to location %s\" % (max_accuracy, prev_model_file_path))\n",
    "    return prev_model_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation\n",
    "Using the function we created above, we now train and evaluate the model.\n",
    "*NOTE: You may see warnings when you run the training due to a [bug in PyTorch](https://github.com/pytorch/pytorch/issues/27972) which is being actively investigated.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating model training\n",
      "Verify if directory `/trained_models` is already exists.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-64e0f312e2b0>\u001b[0m in \u001b[0;36mtrain_and_eval\u001b[0;34m(dd, train_dataset, test_dataset, epoch, model_dir)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_and_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initiating model training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mcreate_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmax_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprev_model_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-a6668e15d6ad>\u001b[0m in \u001b[0;36mcreate_dir\u001b[0;34m(dir_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Verify if directory `%s` is already exists.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdir_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Directory `%s` does not exists.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdir_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating directory `%s` to store trained models.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdir_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "epoch = 30\n",
    "model_dir='/trained_models'\n",
    "model_filepath = train_and_eval(dd, train_dataset, test_dataset, epoch, model_dir)\n",
    "cleanup_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the model generated above, we now score the test dataset against the model to determine if the domain is likely generated by a DGA or benign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = DGADetector()\n",
    "dd.load_model(model_filepath)\n",
    "\n",
    "pred_results = []\n",
    "true_results = []\n",
    "for partition in test_dataset.partitioned_dfs:\n",
    "    pred_results.append(list(dd.predict(partition['domain']).values_host))\n",
    "    true_results.append(list(partition['type'].values_host))\n",
    "pred_results = np.concatenate(pred_results)\n",
    "true_results = np.concatenate(true_results)\n",
    "accuracy_score = accuracy_score(pred_results, true_results)\n",
    "print('Model accuracy: %s'%(accuracy_score))\n",
    "cleanup_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_precision = average_precision_score(true_results, pred_results)\n",
    "\n",
    "print('Average precision score: {0:0.3f}'.format(average_precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DGA detector in CLX enables users to train their models for detection and also use existing models. This capability could also be used in conjunction with log parsing efforts if the logs contain domain names. DGA detection done with CLX and RAPIDS keeps data in GPU memory, removing unnecessary copy/converts and providing a 4X speed advantage over CPU only implementations. This is esepcially true with large batch sizes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
